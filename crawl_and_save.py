#!/usr/bin/env python3
"""ì‹¤ì œ EDGAR ë°ì´í„° í¬ë¡¤ë§ ë° íŒŒì¼ ì €ì¥ ìŠ¤í¬ë¦½íŠ¸"""

import requests
import json
import os
import sys
from pathlib import Path
from datetime import datetime

# src ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent / "src"))

# í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ.setdefault("SUPABASE_URL", "https://test.supabase.co")
os.environ.setdefault("SUPABASE_KEY", "test-key")
os.environ.setdefault("SUPABASE_SERVICE_KEY", "test-service-key") 
os.environ.setdefault("OPENAI_API_KEY", "sk-test")
os.environ.setdefault("USER_AGENT", "ISU JO (ourwavelets@gmail.com)")

from config.settings import settings


class EdgarDataCrawler:
    """EDGAR ë°ì´í„° í¬ë¡¤ë§ ë° ë¡œì»¬ íŒŒì¼ ì €ì¥ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.base_url = "https://data.sec.gov/submissions"
        self.filing_base_url = "https://www.sec.gov/Archives/edgar/data"
        self.headers = {
            "User-Agent": settings.user_agent,
            "Accept": "application/json"
        }
        
        # ë°ì´í„° ì €ì¥ìš© ë””ë ‰í† ë¦¬ ìƒì„±
        self.data_dir = Path("data")
        self.data_dir.mkdir(exist_ok=True)
        
        # íšŒì‚¬ ì •ë³´ ë° íŒŒì¼ë§ë³„ ì„œë¸Œë””ë ‰í† ë¦¬ ìƒì„±
        self.companies_dir = self.data_dir / "companies"
        self.companies_dir.mkdir(exist_ok=True)
        
        self.filings_dir = self.data_dir / "filings"
        self.filings_dir.mkdir(exist_ok=True)
        
    def save_company_info(self, company_data):
        """íšŒì‚¬ ê¸°ë³¸ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        company_file = self.companies_dir / f"{company_data['cik']}_info.json"
        
        with open(company_file, 'w', encoding='utf-8') as f:
            json.dump(company_data, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ’¾ Company info saved: {company_file}")
        
    def download_filing_content(self, cik, accession_number):
        """SEC EDGARì—ì„œ ì‹¤ì œ 10-K íŒŒì¼ë§ ë‚´ìš© ë‹¤ìš´ë¡œë“œ"""
        # CIK ë²ˆí˜¸ì—ì„œ ì•ì˜ 0 ì œê±°í•˜ì—¬ URL ìƒì„±
        clean_cik = str(int(cik))
        
        # SEC EDGAR íŒŒì¼ë§ ë‹¤ìš´ë¡œë“œ URL ìƒì„±
        filing_url = f"{self.filing_base_url}/{clean_cik}/{accession_number.replace('-', '')}/{accession_number}.txt"
        
        try:
            print(f"ğŸ“¥ Downloading: {filing_url}")
            response = requests.get(filing_url, headers=self.headers, timeout=60)
            response.raise_for_status()
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ë§ ë‚´ìš©ì„ ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥
            filing_file = self.filings_dir / f"{cik}_{accession_number}.txt"
            
            with open(filing_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            print(f"ğŸ’¾ Filing saved: {filing_file}")
            print(f"ğŸ“Š File size: {len(response.text):,} characters")
            
            return {
                'success': True,
                'file_path': str(filing_file),
                'content_length': len(response.text),
                'url': filing_url
            }
            
        except Exception as e:
            print(f"âŒ Download failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'url': filing_url
            }
    
    def crawl_company(self, cik, company_name=None):
        """íŠ¹ì • íšŒì‚¬ì˜ ì „ì²´ ë°ì´í„° í¬ë¡¤ë§ ë° ì €ì¥ ì²˜ë¦¬"""
        print(f"\nğŸ¢ Crawling company: {cik}")
        
        # 1ë‹¨ê³„: SEC APIì—ì„œ íšŒì‚¬ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        url = f"{self.base_url}/CIK{cik.zfill(10)}.json"
        
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            company_info = {
                'cik': cik,
                'name': data.get('name', company_name or 'Unknown'),
                'sic': data.get('sic'),
                'sicDescription': data.get('sicDescription'),
                'crawled_at': datetime.now().isoformat(),
                'filings': []
            }
            
            # 2ë‹¨ê³„: ìµœê·¼ 10-K íŒŒì¼ë§ ëª©ë¡ì—ì„œ í•„ìš”í•œ íŒŒì¼ë§ ì°¾ê¸°
            recent_filings = data.get('filings', {}).get('recent', {})
            forms = recent_filings.get('form', [])
            accession_numbers = recent_filings.get('accessionNumber', [])
            filing_dates = recent_filings.get('filingDate', [])
            
            ten_k_count = 0
            for i, form in enumerate(forms):
                if form == '10-K' and ten_k_count < 2:  # ìµœì‹  10-K íŒŒì¼ë§ 2ê°œë§Œ ì²˜ë¦¬
                    if i < len(accession_numbers) and i < len(filing_dates):
                        accession = accession_numbers[i]
                        filing_date = filing_dates[i]
                        
                        # 3ë‹¨ê³„: í•´ë‹¹ íŒŒì¼ë§ì˜ ì‹¤ì œ ë‚´ìš© ë‹¤ìš´ë¡œë“œ
                        download_result = self.download_filing_content(cik, accession)
                        
                        filing_info = {
                            'accessionNumber': accession,
                            'filingDate': filing_date,
                            'form': form,
                            'download_result': download_result
                        }
                        
                        company_info['filings'].append(filing_info)
                        ten_k_count += 1
            
            # 4ë‹¨ê³„: ìˆ˜ì§‘ëœ ëª¨ë“  ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
            self.save_company_info(company_info)
            
            return company_info
            
        except Exception as e:
            print(f"âŒ Company crawling failed: {e}")
            return None


def main():
    """ë©”ì¸ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ EDGAR Data Crawler & Saver")
    print("=" * 60)
    
    # í¬ë¡¤ë§ ëŒ€ìƒ íšŒì‚¬ë“¤ ì •ì˜ (CIK ë²ˆí˜¸, íšŒì‚¬ëª…)
    companies = [
        ("0000320193", "Apple Inc."),
        ("0000789019", "Microsoft Corp."),
        ("0001652044", "Alphabet Inc."),
    ]
    
    crawler = EdgarDataCrawler()
    results = []
    
    for cik, name in companies:
        result = crawler.crawl_company(cik, name)
        results.append(result)
        
        # API ìš”ì²­ ì œí•œì„ ìœ„í•œ ëŒ€ê¸° ì‹œê°„
        import time
        time.sleep(1)
    
    # í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ë° í†µê³„ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š CRAWLING SUMMARY")
    print("=" * 60)
    
    successful = [r for r in results if r is not None]
    total_filings = sum(len(r.get('filings', [])) for r in successful)
    
    print(f"âœ… Companies crawled: {len(successful)}")
    print(f"ğŸ“„ Total filings downloaded: {total_filings}")
    print(f"ğŸ“ Data saved in: {crawler.data_dir.absolute()}")
    
    # ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ëª©ë¡ ë° í¬ê¸° ì •ë³´
    print(f"\nğŸ“‹ Downloaded files:")
    for txt_file in crawler.filings_dir.glob("*.txt"):
        size = txt_file.stat().st_size
        print(f"   ğŸ“„ {txt_file.name} ({size:,} bytes)")
    
    print(f"\nâœ¨ Crawling completed!")


if __name__ == "__main__":
    main()